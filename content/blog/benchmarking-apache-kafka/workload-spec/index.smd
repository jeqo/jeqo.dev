---
.title = "Benchmarking Kafka: Understanding OpenMessaging Benchmark workloads",
.description = "Understand how benchmark periods, throughput, and clients work in OMB to define different workload types available.",
.date = @date("2025-03-15"),
.author = "Jorge Esteban Quilcate Otoya",
.layout = "post.shtml",
.tags = [
  "how-to",
  "apache-kafka",
  "performance",
],
.draft = true,
--- 

In the [previous post](/blog/benchmarking-apache-kafka/intro-omb), we got started with the OpenMessaging Benchmark (OMB) framework. 
Now let's dive into how workload specifications work.
For this need to look into 4 core concepts: Benchmark execution times, Throughput definition, Topic topology, and Producer/Consumer Topology

## Topic topology

In OMB, topic topology is pretty straightforward to configure: you define the number of topics and the number of partitions per topic.

```yaml
topics: 100
partitionsPerTopic: 12
```

In this example, 100 topics are created, each with 12 partitions.

However, this configuration has subtle implications on the benchmark's scheduling and execution.
Let's look into those.

## Producer and consumer topology

The number of producers and consumer groups are defined by the number of topics:

```yaml
# producers
producersPerTopic: 3
# consumers
subscriptionsPerTopic: 3
consumerPerSubscription: 3
```

In this example, each topic will have 3 producers and 3 consumer groups, each with 3 consumers.

But how are these producers and consumers distributed across the workers?

By default, OMB workers are divided in half into producers and consumers.
This means that in a distributed environment, you need at least 2 workers to run a benchmark;
one for producers and one for consumers.

> If you want to distribute producers and consumers across multiple AZs, you will need 2 * AZs workers.

This context is helpful to ensuret that the producer assignments (producers * number of topics) and consumer assignments
(subscription * consumers per subscription * number of topics) are properly distributed across the workers.

> There's not guarantee on the ordering though.
> Each assignment is shuffled before being distributed across the workers.

> There's an option to have extra workers for consumers (2/3 of the workers for consumers and 1/3 for producers), 
using the `-x` flag when running the benchmark.

## Benchmark execution periods

During a benchmark execution, there are three main time periods to consider:

- **Bootstrap time**: This is when the environment is set up, including topic creation and testing producers and consumers can write and read messages.
This phase is usually short and is not part of the benchmark's defined periods.
- **Warm-up time**: Once the topology is set up, the warm-up period begins.
This is the initial period where the system is ramped up to the desired throughput.
This phase is essential to stabilize the system and ensure consistent performance measurements.
It's commonly shorter than the run time but can vary based on the workload type.
By default, the warm-up time is 1 minute.
- **Run time**: The period where the system is running at the desired throughput.
This phase is used to collect performance metrics and evaluate the system's behavior under load.
It should be long enough to capture the system's steady-state performance, and follows the warmup time.

The last two are defined on the workload specification:

```yaml
warmupDurationMinutes: 10
testDurationMinutes: 50
```

The above configuration defines a simple benchmark with a 10-minute warm-up period after initialization and a 50-minute test duration, for a total run of 1 hour.

There's another optional execution time, but it's specific to a workload type, which we'll cover later.

## Throughput definition

Let's also clarify what we mean by _throughput_ in the context of OMB. 
Throughput is defined by the producer rate (messages per second) and the message sizing (in bytes).

In OMB, the producer rate is the number of messages produced per second by _all_ producers.
First the producer rate is evenly distributed across the workers (the ones tasked as producers),
and then each worker uniformly distributes the messages across the producers assigned.

```yaml
producerRate: 1000
```

The message size is defined either by providing an input file or by specifying a randomly generated message with a fixed size.

```yaml
useRandomizedPayloads: true
```

When using a file, the message size is determined by the file's content, and it's provided like this:

```yaml
payloadFile: /path/to/file
```

When using randomized payloads, you can configure the message size and the ratio of random bytes to the total message size.
The `randomizedPayloadPoolSize` is the number of different payloads to generate.

```yaml
messageSize: 1024
randomBytesRatio: 0.8
randomizedPayloadPoolSize: 1000
```

This configuration generates messages with a size of 1024 bytes, where 80% of the bytes are random and the rest are fixed.
1000 different payloads are generated, and each worker will randomly select one of these payloads to send.

Additionally, you can configure what type of message key distribution you want to use:

```yaml
keyDistributor: "NO_KEY" # default, values: NO_KEY, KEY_ROUND_ROBIN, RANDOM_NANO
```

For reference, the available key distributor type descriptions are:

```java
public enum KeyDistributorType {
    /** Key distributor that returns null keys to have default publish semantics. */
    @JsonEnumDefaultValue
    NO_KEY,

    /** Genarate a finite number of "keys" and cycle through them in round-robin fashion. */
    KEY_ROUND_ROBIN,

    /** Random distribution based on System.nanoTime(). */
    RANDOM_NANO,
}
```

## Workload types

Now that benchmark periods and throughput are defined, let's explore the different workload types available in OMB.
Based on my experience with OMB, the workloads can be defined as follows:

While OMB doesn't have an explicit `workloadType` attribute in its configuration, you can configure parameters to create distinctly different benchmark behaviors. 
Understanding these workload patterns is essential for accurately simulating real-world Kafka deployments and uncovering performance characteristics that matter to your specific use case.

- **Fixed throughput**: The producers and consumers aim to maintain a specified (fixed) throughput. This throughput is applied during both warmup and run time.

- **Max throughput**: The producers aren't bounded by a specified throughput limit. Based on observed _backlog_ (`totalPublished - totalReceived`), producers dynamically increase or decrease throughput. 
Ideally, once the maximum sustainable throughput is found, it's maintained while the cluster is tested. During both warm-up and run time, producers continuously adjust as the throughput is evaluated.

- **Consumer backlog**: This mode builds on the fixed throughput approach but expands benchmark phases to three distinct periods: warm-up, backlog building time, and run time.
When building a backlog, consumers are paused, and producers continue to produce messages until a specified backlog size is reached.
It uses the fixed throughput mode to build a backlog of messages before starting the run time phase where consumers process the backlog and then maintain the fixed throughput.

Let's look deeper into each of these workload types.

In my distributed OMB setup I have 6 workers across 3 AZs, 2 workers per AZ.
As a target, I have an Aiven Kafka service with 3 brokers.
They are all in the same region, so the network latency and cost is minimized.

### Fixed throughput

Fixed throughput is the most common workload type and is relatively straightforward to run. 
The goal is to maintain a stable throughput throughout the benchmark execution.

By defining a producer rage higher than zero the workload will be considered as fixed throughput.

```yaml
name: "Fixed Throughput"
# Benchmark periods
testDurationMinutes: 10

# Topic topology
topics: 10
partitionsPerTopic: 12

# Throughput
producerRate: 20000
messageSize: 1024
useRandomizedPayloads: true
randomBytesRatio: 0.9
randomizedPayloadPoolSize: 1000

# Producers and consumers
producersPerTopic: 3
subscriptionsPerTopic: 3
consumerPerSubscription: 3
```

Here's is how the cluster load would look like:

// TODO add image

### Maximum throughput

By defining a producer rate of zero, the workload will be considered as max throughput.
The initial producer rate is defined at 10K messages per second, and then it will adjust based on the observed latencies.

```yaml
name: "Max Throughput"
# Benchmark periods
testDurationMinutes: 10

# Topic topology
topics: 10
partitionsPerTopic: 12

# Throughput
producerRate: 0 # max throughput
messageSize: 1024
useRandomizedPayloads: true
randomBytesRatio: 0.9
randomizedPayloadPoolSize: 1000

# Producers and consumers
producersPerTopic: 3
subscriptionsPerTopic: 3
consumerPerSubscription: 3
```

Here's is how the cluster load would look like:

// TODO add image

### Consumer backlog

Consumer backlog is a more niche workload type that simulates the behavior of a cluster when older data needs to be reprocessed.
It builds on the fixed throughput mode but adds a backlog-building phase before the run time.

```yaml
name: "Consumer Backlog"
# Benchmark periods
testDurationMinutes: 10

# Topic topology
topics: 10
partitionsPerTopic: 12

# Throughput
producerRate: 20000
messageSize: 1024
useRandomizedPayloads: true
randomBytesRatio: 0.9
randomizedPayloadPoolSize: 1000

# Producers and consumers
producersPerTopic: 3
subscriptionsPerTopic: 3
consumerPerSubscription: 3

# Consumer backlog
consumerBacklogSizeGB: 10
```

Here's is how the cluster load would look like:

// TODO add image

The main challenge I've found with this mode is that it's based on the fixed rate, so if you want to build a large backlog, you'll have to wait linearly based on the fixed throughput.
In my wish list, I'd like to have two throughput modes: 
one to build the backlog (e.g., max or just a high throughput) and another to maintain throughput while consuming the backlog.

---

Additional ideas:
Something I haven't play much with yet is with a mixed benchmark where the modes can be mixed: so you could run a fixed thoruhgput let's say to simulate the current load on a cluster, and while that is running have another OMB cluster running any other of the modes to explore how an existing cluster would behave when increasing the load.

Of course, these workload models can only take you so far as there many other things that are hard to test, like number of connections, reconnections, mis-configured clients, having multiple applications with a mix of configurations that may stress the cluster in specific ways.

Nevertheless, the OMB can help to simulate a good bunch of scenarios, mix and match their workload types, and apply operations on top to test certain behaviors, e.g. what happen when a rolling restart happens in the middle of a benchmark; what if we enable TS for all the topics while running a benchmark; or some other specific example.

In a following post I'd like to explore a bit more how does the topic and client topology works in OMB; and how it does get distributed across OMB workers.

