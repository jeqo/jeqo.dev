---
.title = "Benchmarking Kafka: Understanding OpenMessaging Benchmark workloads",
.description = "Understand how benchmark periods, throughput, and clients work in OMB to define different workload types available.",
.date = @date("2025-03-15"),
.author = "Jorge Esteban Quilcate Otoya",
.layout = "post.shtml",
.tags = [
  "how-to",
  "apache-kafka",
  "performance",
],
.draft = true,
--- 

In the [previous post](/blog/benchmarking-apache-kafka/intro-omb), OpenMessaging Benchmark (OMB) framework was presented and how to run it locally. 
Now let's dive into how workload specifications are defined and their different execution modes.
For this need to look into 4 core concepts: Benchmark execution times, Throughput definition, Topic topology, and Producer/Consumer Topology

## Topic topology

In OMB, topic topology is pretty straightforward to configure: you define the number of topics and the number of partitions per topic.

```yaml
topics: 100
partitionsPerTopic: 12
```

In this example, 100 topics are created, each with 12 partitions.

However, this configuration has subtle implications on the benchmark's scheduling and execution.
Let's look into those.

## Producer and consumer topology

The number of producers and consumer groups are defined by the number of topics:

```yaml
# producers
producersPerTopic: 3
# consumers
subscriptionsPerTopic: 3
consumerPerSubscription: 3
```

In this example, each topic will have 3 producers and 3 consumer groups, each with 3 consumers.

But how are these producers and consumers distributed across the workers?

By default, OMB workers are divided in half into producers and consumers.
This means that in a distributed environment, you need at least 2 workers to run a benchmark;
one for producers and one for consumers.

> If you want to distribute producers and consumers across multiple AZs, you will need 2 * AZs workers.

This context is helpful to ensuret that the producer assignments (producers * number of topics) and consumer assignments
(subscription * consumers per subscription * number of topics) are properly distributed across the workers.

> There's not guarantee on the ordering though.
> Each assignment is shuffled before being distributed across the workers.

> There's an option to have extra workers for consumers (2/3 of the workers for consumers and 1/3 for producers), 
using the `-x` flag when running the benchmark.

## Benchmark execution periods

During a benchmark execution, there are three main time periods to consider:

- **Bootstrap time**: This is when the environment is set up, including topic creation and testing producers and consumers can write and read messages.
This phase is usually short and is not part of the benchmark's defined periods.
- **Warm-up time**: Once the topology is set up, the warm-up period begins.
This is the initial period where the system is ramped up to the desired throughput.
This phase is essential to stabilize the system and ensure consistent performance measurements.
It's commonly shorter than the run time but can vary based on the workload type.
By default, the warm-up time is 1 minute.
- **Run time**: The period where the system is running at the desired throughput.
This phase is used to collect performance metrics and evaluate the system's behavior under load.
It should be long enough to capture the system's steady-state performance, and follows the warmup time.

The last two are defined on the workload specification:

```yaml
warmupDurationMinutes: 10
testDurationMinutes: 50
```

The above configuration defines a simple benchmark with a 10-minute warm-up period after initialization and a 50-minute test duration, for a total run of 1 hour.

There's another optional execution time, but it's specific to a workload type, which we'll cover later.

## Throughput definition

Let's also clarify what we mean by _throughput_ in the context of OMB. 
Throughput is defined by the producer rate (messages per second) and the message sizing (in bytes).

In OMB, the producer rate is the number of messages produced per second by _all_ producers.
First the producer rate is evenly distributed across the workers (the ones tasked as producers),
and then each worker uniformly distributes the messages across the producers assigned.

```yaml
producerRate: 1000
```

The message size is defined either by providing an input file or by specifying a randomly generated message with a fixed size.

```yaml
useRandomizedPayloads: true
```

When using a file, the message size is determined by the file's content, and it's provided like this:

```yaml
payloadFile: /path/to/file
```

When using randomized payloads, you can configure the message size and the ratio of random bytes to the total message size.
The `randomizedPayloadPoolSize` is the number of different payloads to generate.

```yaml
messageSize: 1024
randomBytesRatio: 0.8
randomizedPayloadPoolSize: 1000
```

This configuration generates messages with a size of 1024 bytes, where 80% of the bytes are random and the rest are fixed.
1000 different payloads are generated, and each worker will randomly select one of these payloads to send.

Additionally, you can configure what type of message key distribution you want to use:

```yaml
keyDistributor: "NO_KEY" # default, values: NO_KEY, KEY_ROUND_ROBIN, RANDOM_NANO
```

For reference, the available key distributor type descriptions are:

```java
public enum KeyDistributorType {
    /** Key distributor that returns null keys to have default publish semantics. */
    @JsonEnumDefaultValue
    NO_KEY,

    /** Genarate a finite number of "keys" and cycle through them in round-robin fashion. */
    KEY_ROUND_ROBIN,

    /** Random distribution based on System.nanoTime(). */
    RANDOM_NANO,
}
```

## Workload types

Now that benchmark periods and throughput are defined, let's explore the different workload types available in OMB.
Based on my experience with OMB, the workloads can be defined as follows:

While OMB doesn't have an explicit `workloadType` attribute in its configuration, you can configure parameters to create distinctly different benchmark behaviors. 
Understanding these workload patterns is essential for accurately simulating real-world Kafka deployments and uncovering performance characteristics that matter to your specific use case.

- **Fixed throughput**: The producers and consumers aim to maintain a specified (fixed) throughput. This throughput is applied during both warmup and run time.

- **Max throughput**: The producers aren't bounded by a specified throughput limit. Based on observed _backlog_ (i.e. lag, `totalMsgsPublished - totalMsgsReceived`), producers dynamically increase or decrease throughput. 
Ideally, once the maximum sustainable throughput is found, it's maintained while the cluster is tested. During both warm-up and run time, producers continuously adjust as the throughput is evaluated.

- **Consumer backlog**: This mode builds on the fixed throughput approach but expands benchmark phases to three distinct periods: warm-up, backlog building time, and run time.
When building a backlog, consumers are paused, and producers continue to produce messages until a specified backlog size is reached.
It uses the fixed throughput mode to build a backlog of messages before starting the run time phase where consumers process the backlog and then maintain the fixed throughput.

Let's look deeper into each of these workload types.

In my distributed OMB setup I have 6 workers across 3 AZs, 2 workers per AZ.
As a target, I have an Aiven Kafka service with 3 brokers.
They are all in the same region, so the network latency and cost is minimized.

### Fixed throughput

Fixed throughput is the most common workload type and is relatively straightforward to run. 
The goal is to maintain a stable throughput throughout the benchmark execution.

By defining a producer rage higher than zero the workload will be considered as fixed throughput.

```yaml
name: "Fixed Throughput"
# Benchmark periods
testDurationMinutes: 10

# Topic topology
topics: 10
partitionsPerTopic: 12

# Throughput
producerRate: 20000
messageSize: 1024
useRandomizedPayloads: true
randomBytesRatio: 0.9
randomizedPayloadPoolSize: 1000

# Producers and consumers
producersPerTopic: 3
subscriptionsPerTopic: 3
consumerPerSubscription: 3
```

Here is how the cluster load would look like:

[]($image.asset('fixed-throughput.png'))

As you could observe, the ingress throughput is growing at warm-up time (1min) and stabilizes on ~20MB/s at run time.
Similarly, egress throughput grows until ~60MB/s (20MB/s ingress * 3 consumers per subscription) and stabilizes until the benchmark finishes.

From the results, it looks like the broker can handle the load, but it's worth checking the broker's metrics to see if it's not hitting any limits.

### Maximum throughput

Fixed throughput is good when a known throughput is expected, but what if you want to test the cluster's maximum throughput for a given configuration?

By defining a producer rate of zero, the workload will be considered as max throughput.
The initial producer rate is defined at 10K messages per second, and then it will adjust based on the observed latencies.

```yaml
name: "Max Throughput"
# Benchmark periods
testDurationMinutes: 10

# Topic topology
topics: 10
partitionsPerTopic: 12

# Throughput
producerRate: 0 # max throughput
messageSize: 1024
useRandomizedPayloads: true
randomBytesRatio: 0.9
randomizedPayloadPoolSize: 1000

# Producers and consumers
producersPerTopic: 3
subscriptionsPerTopic: 3
consumerPerSubscription: 3
```

Here is how the cluster load would look like:

[]($image.asset('max-throughput.png'))

This is a more dynamic workload type where the producers adjust their throughput based on the observed lags.
The ingress throughput grow and stabilize ~325MB/s, while the egress throughput grows and stabilizes ~975MB/s.

This mode is useful to understand the cluster's limits and how it behaves under different loads.

### Consumer backlog

Consumer backlog is a more niche workload type that simulates the behavior of a cluster when older data needs to be reprocessed.
It builds on the fixed throughput mode but adds a backlog-building phase before the run time.

```yaml
name: "Consumer Backlog"
# Benchmark periods
testDurationMinutes: 10

# Topic topology
topics: 10
partitionsPerTopic: 12

# Throughput
producerRate: 20000
messageSize: 1024
useRandomizedPayloads: true
randomBytesRatio: 0.9
randomizedPayloadPoolSize: 1000

# Producers and consumers
producersPerTopic: 3
subscriptionsPerTopic: 3
consumerPerSubscription: 3

# Consumer backlog
consumerBacklogSizeGB: 10
```

Here is how the cluster load would look like:

[]($image.asset('consumer-backlog.png'))

In this mode, there is the additional period of backlog building, where the producers continue to produce messages until the backlog size is reached.
So, there is the initial warm-up of 1 minute at 20MB/s in, 60MB/s out; then the consumers are turned off, and the producers continue to produce messages until the backlog size is reached.
When the backlog is reached, the consumers are turned on, and the producers maintain the fixed throughput.

This mode is useful to understand how the cluster behaves when there is a need to reprocess older data or hit "colder" data, either on disk, or on remote storage when Tiered Storage is enabled.

The main challenge I've found with this mode is that it's based on the fixed rate, so if you want to build a large backlog, you'll have to wait linearly based on the fixed throughput.
In my wish list, I'd like to have two throughput modes: 
one to build the backlog (e.g., max or just a high throughput) and another to maintain throughput while consuming the backlog.

## Summary

In this post, we've explored how to define different workload types in OMB, focusing on fixed throughput, max throughput, and consumer backlog.
Each workload type has its own characteristics and is useful for different testing scenarios.

By understanding how to configure benchmark periods, throughput, topic topology, and producer/consumer topology, you can create workloads that simulate real-world scenarios and uncover performance characteristics that matter to your specific use case.

These workloads could even be mixed to simulate more complex scenarios, like a fixed throughput benchmark running while a max throughput benchmark is also running, or a consumer backlog benchmark running while a fixed throughput benchmark is running.

Keep in mind that each of these workloads are highly dependent on the topic and client topology, and how they are tuned and distributed across the OMB workers.

