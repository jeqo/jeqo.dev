---
.title = "Benchmarking Kafka: Understanding OpenMessaging Benchmark workloads",
.description = "Understand how benchmark periods, throughput, and clients work in OMB to define different workload types available.",
.date = @date("2025-03-15"),
.author = "Jorge Esteban Quilcate Otoya",
.layout = "post.shtml",
.tags = [
  "how-to",
  "apache-kafka",
  "performance",
],
.draft = true,
--- 

In the [previous post](/blog/benchmarking-apache-kafka/intro-omb), we got started with the OpenMessaging Benchmark (OMB) framework. 
Now let's dive into the types of workloads that can be modeled.

Before we dive into the workload types, let's look into how a Workload is specified in OMB.
For this need to look into 4 core concepts: Benchmark execution times, Throughput definition, Topic topology, and Producer/Consumer Topology

## Topic topology

In OMB, topic topology is pretty straighforward to configure: you define the number of topics and the number of partitions per topic.

```yaml
topics: 100
partitionsPerTopic: 12
```

In this example, 100 topics are created, each with 12 partitions.

However, this configuration has subtle implications on the benchmark's scheduling and execution.

## Producer and consumer topology

The number of producers and consumer groups are defined by the number of topics:

```yaml
# producers
producersPerTopic: 3
# consumers
subscriptionsPerTopic: 3
consumerPerSubscription: 3
```

In this example, each topic will have 3 producers and 3 consumer groups, each with 3 consumers.

Then, these producers have to be distributed across the workers.
By default, OMB workers are divided in half into producers and consumers.
This means that in a distributed environment, you need at least 2 workers to run a benchmark;
one for the producers and one for the consumers.

> If you want to distribute producers and consumers across multiple AZs, you will need 2 * AZs workers.

Knowing this, ensure that the producer assignments (producers * number of topics) and consumer assignments
(suscription * consumers per suscription * number of topics) are properly distributed across the workers.

> There's an option to have extra workers for consumers (2/3 of the workers for consumers and 1/3 for producers), 
using the `-x` flag when running the benchmark.

At this point, we have a clear understanding of the topic and client topology, and how these are distributed across the workers.

## Benchmark execution periods

Benchmark executions have these main periods:

- **Bootstrap time**: This is when the environment is set up, including topic creation, and testing producers and consumers can write and read messages.
This phase is usually short and is not part of the benchmark's defined periods.
- **Warm-up time**: Once the topology is set up, the warm-up period begins.
This is the initial period where the system is ramped up to the desired throughput.
This phase is essential to stabilize the system and ensure consistent performance measurements.
It's commonly shorter than the run time but can vary based on the workload type.
- **Run time**: The period where the system is running at the desired throughput.
This phase is used to collect performance metrics and evaluate the system's behavior under load.
It should be long enough to capture the system's steady-state performance, and follows the warmup time.

The last two are defined on the workload specification:

```yaml
warmupDurationMinutes: 10
testDurationMinutes: 50
```

The above configuration defines a simple benchmark with a 10-minute warm-up period after initialization and a 50-minute test duration, for a total run of 1 hour.

There's another optional execution time, but it's specific to a workload type, which we'll cover later.

## Throughput definition

Let's also clarify what we mean by "throughput" in the context of OMB. 
Throughput is defined by the producer rate (messages per second) and the message sizing (in bytes).

In OMB, the producer rate is the number of messages produced per second by _all_ producers.
First the producer rate is evenly distributed across the workers (the ones tasked as producers),
and then each worker uniformly distributes the messages across the producers assigned.

```yaml
producerRate: 1000
```

The message size is defined either by providing an input file or by specifying a randomly generated message with a fixed size.

```yaml
useRandomizedPayloads: true
```

When using a file, the message size is determined by the file's content, and it's provided like this:

```yaml
payloadFile: /path/to/file
```

When using randomized payloads, you can configure the message size and the ratio of random bytes to the total message size.
The `randomizedPayloadPoolSize` is the number of different payloads to generate.

```yaml
messageSize: 1024
randomBytesRatio: 0.8
randomizedPayloadPoolSize: 1000
```

This configuration generates messages with a size of 1024 bytes, where 80% of the bytes are random and the rest are fixed.
1000 different payloads are generated, and each worker will randomly select one of these payloads to send.

## Workload types

Now that benchmark periods and throughput are defined, let's explore the different workload types available in OMB.
Based on my experience with OMB, the workloads can be defined as follows:

While OMB doesn't have an explicit `workloadType` attribute in its configuration, you can configure parameters to create distinctly different benchmark behaviors. 
Understanding these workload patterns is essential for accurately simulating real-world Kafka deployments and uncovering performance characteristics that matter to your specific use case.

- **Fixed throughput**: The producers and consumers aim to maintain a specified (fixed) throughput. This throughput is applied during both warmup and run time.

- **Max throughput**: The producers aren't bounded by a specified throughput limit. Based on observed _backlog_ (`totalPublished - totalReceived`), producers dynamically increase or decrease throughput. 
Ideally, once the maximum sustainable throughput is found, it's maintained while the cluster is tested. During both warm-up and run time, producers continuously adjust as the throughput is evaluated.

- **Consumer backlog**: This mode builds on the fixed throughput approach but expands benchmark phases to three distinct periods: warm-up, backlog building time, and run time.
When building a backlog, consumers are paused, and producers continue to produce messages until a specified backlog size is reached.
It uses the fixed throughput mode to build a backlog of messages before starting the run time phase where consumers process the backlog and then maintain the fixed throughput.

### Fixed throughput

Fixed throughput is the most common workload type and is relatively straightforward to run. 
The goal is to maintain a stable throughput throughout the benchmark execution.

By defining a producer rage higher than zero the workload will be considered as fixed throughput.

### Maximum throughput

By defining a producer rate of zero, the workload will be considered as max throughput.
The initial producer rate is defined at 10K messages per second, and then it will adjust based on the observed latencies.


### Consumer backlog

Consumer backlog is a more niche workload type that simulates the behavior of a cluster when older data needs to be reprocessed.
It builds on the fixed throughput mode but adds a backlog-building phase before the run time.

The main challenge with this mode is that it's based on the fixed mode, so if you want to build a large backlog, you'll have to wait linearly based on the fixed throughput.
In my wish list, I'd like to have two throughput modes: one to build the backlog (e.g., max or just a high throughput) and another to maintain throughput while consuming the backlog.

---

Additional ideas:
Something I haven't play much with yet is with a mixed benchmark where the modes can be mixed: so you could run a fixed thoruhgput let's say to simulate the current load on a cluster, and while that is running have another OMB cluster running any other of the modes to explore how an existing cluster would behave when increasing the load.

Of course, these workload models can only take you so far as there many other things that are hard to test, like number of connections, reconnections, mis-configured clients, having multiple applications with a mix of configurations that may stress the cluster in specific ways.

Nevertheless, the OMB can help to simulate a good bunch of scenarios, mix and match their workload types, and apply operations on top to test certain behaviors, e.g. what happen when a rolling restart happens in the middle of a benchmark; what if we enable TS for all the topics while running a benchmark; or some other specific example.

In a following post I'd like to explore a bit more how does the topic and client topology works in OMB; and how it does get distributed across OMB workers.

