---
.title = "Benchmarking Kafka: Distributed Workers and Workload topology in OpenMessaging Benchmark",
.description = "Dive into OpenMessaging Benchmark's distributed mode and how clients are deployed across multiple workers",
.date = @date("2025-03-30"),
.author = "Jorge Esteban Quilcate Otoya",
.layout = "post.shtml",
.tags = [
  "how-to",
  "apache-kafka",
  "performance",
],
.draft = true,
--- 

In the [previous post](/blog/benchmarking-apache-kafka/intro-omb), OpenMessaging Benchmark (OMB) framework was presented and how to run it locally.

Now let's dive into the distributed mode, how are workers deployed across multiple nodes, and how to define the workload topology.

In local mode, all components run in a single process, which is ideal for getting started and initial testing. 
However, for production-like performance testing, we need to deploy workers across multiple nodes to achieve higher throughput, and represent better real-world scenarios.

The distributed mode consists of deploying workers across multiple nodes, each exposing a REST endpoint. 
The benchmark CLI uses these endpoints to coordinate the workload execution (e.g. create producers on this worker, create consumers on this other one).

# Provisioning Workers

OMB workers running in distributed mode are Java server processes that expose a REST API to receive commands from the benchmark CLI to run workload assignments (i.e. producers and consumers).

These workers can be deployed in any environment, as long as they can access the Kafka cluster.
The preferred way is to run them as VMs or containers in the same network (e.g. same cloud region, with VPC peerig, and to reduce inter-AZ costs: with access to private IPs[[4](https://blog.2minutestreaming.com/p/basic-aws-networking-costs)]) as the Kafka cluster to minimize network latency.
The OMB repository provides Terraform scripts[[1](https://github.com/openmessaging/benchmark/blob/master/driver-kafka/deploy/hdd-deployment)] 
to deploy workers on AWS, and Alibaba Cloud; 
and Ansible playbooks[[2](https://github.com/openmessaging/benchmark/blob/master/driver-kafka/deploy/hdd-deployment/deploy.yaml)] to install the required dependencies and start the workers.

Both include deploying a Kafka cluster and Zookeeper ensemble, and the workers themselves.
In my experience, this is useful as a template to start with, but you may need to adjust the scripts to fit your environment (e.g. remove Kafka cluster deployment as you may already have one to test, adjust versions, instrumenting workers with metrics agents and profilers, etc.).

As a reference, I have created a simple Docker image for OMB workers based on this scripts to test this locally as a Docker Compose setup: https://github.com/jeqo/docker-composes/tree/main/openmessaging-benchmark

Once deployed, the Workers are standalone processes that expose 2 ports: one for the REST API to schedule assignments, and another for stats collection.

```bash
./bin/benchmark-worker \
    -p 8080 \
    -sp 8081
```

The CLI will have to provide a list of workers to use for the benchmark:

```bash
./bin/benchmark \
    --workers http://worker1:8080,http://worker2:8080 \
    --driver driver.yaml \
    workload.yaml
```

## Workers distribution

Any benchmark workload includes a set of producers and consumers, and these need to be assigned to specific workers. This means that a Worker is either a producer or a consumer, but not both.

So there's the first limitation when trying to run a workload on a distributed setup: the minimum number of workers is 2: one for producers and another for consumers.
And the number of workers will be split in half between producers and consumers---unless you have a specific type of workload that requires more consumers than producers, where you can assign extra workers to consumers (2/3 consumers, 1/3 producers)[[3](https://github.com/openmessaging/benchmark/blob/8f7d5d65ef63d87140b5908945df4b2bfdd4645a/benchmark-framework/src/main/java/io/openmessaging/benchmark/worker/DistributedWorkersEnsemble.java#L67-L74)].

### Rack-awareness

If running clusters on a single rack (e.g. in a single Availability Zone), the default setup may suffice to distribute the load across workers.
However, if running across multiple zones, you may want to deploy workers in a rack-aware manner to minimize network latency and costs.
If trying Kafka-compatible systems that support leaderless writes with AZ-awareness (e.g. Warpstream[[5](https://docs.warpstream.com/warpstream/byoc/configure-kafka-client/configure-clients-to-eliminate-az-networking-costs)]),
you may want to flag workers with the zone they are running in to pass this information to the Kafka clients.

There is a hidden (i.e. undocumented) feature in OMB to define a `zone.id` property:

```bash
[Service]
ExecStart=/opt/benchmark/bin/benchmark-worker
Environment='JVM_OPTS=-Dzone.id={{ az }}'
```

That is used to pass the zone from the worker process to the `client.id` property:

If on the Driver configuration you have a `client.id` property:

```yaml
name: kafka-local
driverClass: io.openmessaging.benchmark.driver.kafka.KafkaBenchmarkDriver

# Kafka client-specific configuration
commonConfig: |
  bootstrap.servers=localhost:9092
  client.id=omb-client_az={zone.id}
producerConfig: ""
consumerConfig: |
  auto.offset.reset=earliest
```

The framework will replace `{zone.id}` with the value of the `zone.id` property from the worker process[[9](https://github.com/openmessaging/benchmark/blob/b10b22767f8063321c90bc9ee1b0aadc5902c31a/driver-kafka/src/main/java/io/openmessaging/benchmark/driver/kafka/KafkaBenchmarkDriver.java#L69-L74)].

The Kafka broker would need to know how to extract this information and use it for client AZ-awareness.

OMB could be extended to pass this information to the `client.rack` as well.
This would allow testing another Kafka features like Follower Fetching, and---if KIP-1123[[6](https://cwiki.apache.org/confluence/display/KAFKA/KIP-1123%3A+Rack-aware+partitioning+for+Kafka+Producer)]
is adopted---Producer rack-awareness.

In this scenario, consider having `N * zones` workers, where N is the number of workers per zone (at least 2).

## Workload topology

### Topic topology

In OMB, topic topology is pretty straightforward to configure: you define the number of topics and the number of partitions per topic.

```yaml
topics: 100
partitionsPerTopic: 12
```

In this example, 100 topics are created, each with 12 partitions.

However, this configuration has subtle implications on the benchmark's scheduling and execution.
Let's look into those.

### Producer and consumer topology

The number of producers and consumer groups are defined by the number of topics:

```yaml
# producers
producersPerTopic: 3
# consumers
subscriptionsPerTopic: 3
consumerPerSubscription: 3
```

In this example, each topic will have 3 producers and 3 consumer groups, each with 3 consumers.

Knowing that workers are split in half between producers and consumers is helpful to ensure that the producer assignments (producers * number of topics) and consumer assignments
(subscription * consumers per subscription * number of topics) are properly distributed across the workers.

> There is not guarantee on assignment ordering though.
> Each assignment is shuffled before being distributed across the workers[[7](https://github.com/openmessaging/benchmark/blob/b10b22767f8063321c90bc9ee1b0aadc5902c31a/benchmark-framework/src/main/java/io/openmessaging/benchmark/WorkloadGenerator.java#L256-L263)][[8](https://github.com/openmessaging/benchmark/blob/b10b22767f8063321c90bc9ee1b0aadc5902c31a/benchmark-framework/src/main/java/io/openmessaging/benchmark/WorkloadGenerator.java#L231-L244)].



## References

* [1]: Terraform scripts: https://github.com/openmessaging/benchmark/blob/master/driver-kafka/deploy/hdd-deployment See `provision-kafka-aws.tf` and `alicloud/provision-kafka-alicloud.tf`
* [2]: Ansible playbook: https://github.com/openmessaging/benchmark/blob/master/driver-kafka/deploy/hdd-deployment/deploy.yaml
* [3]: Code snippet for defining number of producer workers: https://github.com/openmessaging/benchmark/blob/8f7d5d65ef63d87140b5908945df4b2bfdd4645a/benchmark-framework/src/main/java/io/openmessaging/benchmark/worker/DistributedWorkersEnsemble.java#L67-L74
* [4]: Basic AWS networking costs: https://blog.2minutestreaming.com/p/basic-aws-networking-costs
* [5]: Warpstream AZ networking costs: https://docs.warpstream.com/warpstream/byoc/configure-kafka-client/configure-clients-to-eliminate-az-networking-costs
* [6]: KIP-1123: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1123%3A+Rack-aware+partitioning+for+Kafka+Producer
* [7]: Producer assignment shuffling: https://github.com/openmessaging/benchmark/blob/b10b22767f8063321c90bc9ee1b0aadc5902c31a/benchmark-framework/src/main/java/io/openmessaging/benchmark/WorkloadGenerator.java#L256-L263
* [8]: Consumer assignment shuffling: https://github.com/openmessaging/benchmark/blob/b10b22767f8063321c90bc9ee1b0aadc5902c31a/benchmark-framework/src/main/java/io/openmessaging/benchmark/WorkloadGenerator.java#L231-L244
* [9]: Client ID zone parsing: https://github.com/openmessaging/benchmark/blob/b10b22767f8063321c90bc9ee1b0aadc5902c31a/driver-kafka/src/main/java/io/openmessaging/benchmark/driver/kafka/KafkaBenchmarkDriver.java#L69-L74

