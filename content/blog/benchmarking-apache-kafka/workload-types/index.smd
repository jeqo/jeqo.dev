---
.title = "Benchmarking Kafka: OpenMessaging Benchmark workload types",
.description = "Use OMB to explore Kafka performance under different types of workloads.",
.date = @date("2025-02-05"),
.author = "Jorge Esteban Quilcate Otoya",
.layout = "post.shtml",
.tags = [
  "how-to",
  "apache-kafka",
  "performance",
],
.draft = true,
--- 

In the [previous post](/blog/benchmarking-apache-kafka/intro-omb), we got started with the OpenMessaging Benchmark (OMB) framework. Now let's dive into the types of workloads that can be modeled.

While OMB doesn't have an explicit `workloadType` attribute in its configuration, you can configure parameters to create distinctly different benchmark behaviors. Understanding these workload patterns is essential for accurately simulating real-world Kafka deployments and uncovering performance characteristics that matter to your specific use case.

Before we dive into the workload types, let's briefly define the main benchmark execution times:

- **Warmup time**: The initial period where the system is ramped up to the desired throughput. This phase is essential to stabilize the system and ensure consistent performance measurements. It's usually shorter than the run time but can vary based on the workload type.
- **Run time**: The period where the system is running at the desired throughput. This phase is used to collect performance metrics and evaluate the system's behavior under load. It should be long enough to capture the system's steady-state performance, and follows the warmup time.

Now let's explore the different workload types available in OMB.
Based on my experience with OMB, the workloads can be defined as follows:

- **Fixed throughput**: The producers and consumers aim to maintain a specified (fixed) throughput. This throughput is applied during both warmup and run time.

- **Max throughput**: The producers aren't bounded by a specified throughput limit. Based on observed latencies (TODO something to cover later in the post), producers dynamically increase or decrease throughput. Ideally, once the maximum sustainable throughput is found, it's maintained while the cluster is tested. During both warmup and run time, producers continuously adjust as the maximum throughput is determined.

- **Consumer backlog**: This mode builds on the fixed throughput approach but expands benchmark phases to three distinct periods: warmup, backlog building time, and run time. It uses the fixed throughput mode to build a backlog of messages before starting the run time phase where consumers process the backlog and then maintain the fixed throughput.

In this post, we'll explore each workload type in detail, including configuration examples, best practices, and real-world applications.

## Throughput definition

Before diving into the workload types, let's clarify what we mean by "throughput" in the context of OMB. 
Throughput is defined by the producer rate (messages per second) and the message sizing (in bytes).
Let's look into how these are defined in the OMB configuration.





## Fixed throughput

Fixed throughput is the most common workload type and is relatively straightforward to run. 
The goal is to maintain a stable throughput throughout the benchmark execution.

    
---

Based on my experience with OMB, the workloads can be defined as follows:

- Fixed throughput: The producers and consumers will aim to cope with a specified (fixed) throughput. This throuhgput will be applied during warmup and run time.
- Max throughput: The producers will not be bounded a specified throughput. Based on the latencies, the producers will increase or decrease its throughput. Ideally, once found, the maximum throughput will be maintained and the cluster will be tested during that scenario. In this case during the warmup time and run time the producers will be dynamically tuned while the max thorughput is found.
- Consumer backlog: This mode is based on the fixed throuhgput mode, and in this case the benchmark times are expanded to 3: warmup, backlog building time, and run time.

So let's see each type in more detailed

Fixed throuhgput is the most common one and easier to run as the goal is specific and tend to be stable.

Max throuhgput is the one that would be more interesting for anyone trying to measure the maximum load to be handled under a certain topology.

Consumer backlog is probably the most niche one, but still interesting if you'd like to know how the cluster will behave when older data needs to be reprocessed.
The main challenge that I've found with this mode is that is based on the fixed mode, so if you'd like to build a large backlog, there's no way around that wait for linear time based on the fixed throuhgput.
In my wish list is to have 2 thorughput modes, one to build the backlog (e.g. max or just a high thourhpgut) and another thoruhgput to keep while consuming the backlog.

Other use-cases:
Something I haven't play much with yet is with a mixed benchmark where the modes can be mixed: so you could run a fixed thoruhgput let's say to simulate the current load on a cluster, and while that is running have another OMB cluster running any other of the modes to explore how an existing cluster would behave when increasing the load.

Of course, these workload models can only take you so far as there many other things that are hard to test, like number of connections, reconnections, mis-configured clients, having multiple applications with a mix of configurations that may stress the cluster in a specific way.

Nevertheless the OMB can help to simulate a good bunch of scenarios, and you mix and match their workload types, and apply operations on top to test certain behaviors, e.g. what happen when a rolling restart happens in the middle of a benchmark; what if we enable TS for all the topics while running a benchmark; or some other specific example.

In a following post I'd like to explore a bit more how does the topic and client topology works in OMB; and how it does get distributed across OMB workers.

